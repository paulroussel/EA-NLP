{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\auges\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import copy\n",
    "import os\n",
    "from cache import Cache\n",
    "from preprocessing_data import vec2string, lemmatizer, stemming, remove_stopwords, tokenization, remove_punct\n",
    "\n",
    "cache=Cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste =[]\n",
    "i=1\n",
    "while os.path.exists(f'../Database/Database/diversified_{i}.json'):\n",
    "    with open(f'../Database/Database/diversified_{i}.json') as file:\n",
    "        liste += json.load(file)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in liste:\n",
    "    elem['date'] = \",\".join([str(e) for e in elem['date']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(liste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(x):\n",
    "    \n",
    "    return (\n",
    "    vec2string(\n",
    "        lemmatizer(\n",
    "            stemming(\n",
    "                remove_stopwords(\n",
    "                    tokenization(\n",
    "                        remove_punct(x).lower()\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_preprocessing(df,nom_colonne):\n",
    "    \n",
    "    df[nom_colonne] = df[nom_colonne].apply(lambda x : preprocessing(x))\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cache.check('clean_df_complete'):\n",
    "    clean_df_complete = cache.load('clean_df_complete')\n",
    "else : \n",
    "    clean_df_complete = apply_preprocessing(df,'content')\n",
    "    clean_df_complete = apply_preprocessing(clean_df_complete,'title')\n",
    "    cache.save(clean_df_complete,'clean_df_complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import gensim\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "WORD2VEC_DIMENSION = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cache.check('content_w2v'):\n",
    "    content_w2v = cache.load('content_w2v')\n",
    "else : \n",
    "    content_w2v = Word2Vec(clean_df_complete['content'], size=WORD2VEC_DIMENSION, window=5, min_count=4, workers=10)\n",
    "    content_w2v.train(clean_df_complete['content'], epochs=500, total_examples=content_w2v.corpus_count)\n",
    "    cache.save(content_w2v,'content_w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embedding(data, w2v):\n",
    "    vectors = []\n",
    "    for c in data:\n",
    "        vec = np.zeros(WORD2VEC_DIMENSION)\n",
    "        nb = 0\n",
    "        for w in c:\n",
    "            if w in w2v.wv:\n",
    "                nb += 1\n",
    "                vec += w2v.wv[w]\n",
    "\n",
    "        if nb:\n",
    "            vec /= nb\n",
    "        vectors.append(vec)\n",
    "        \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(361835047, 2020961000)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if cache.check('title_w2v'):\n",
    "    title_w2v = cache.save('title_w2v')\n",
    "else:\n",
    "    title_w2v = Word2Vec(clean_df_complete['title'], size=WORD2VEC_DIMENSION, window=5, min_count=4, workers=10)\n",
    "    title_w2v.train(clean_df_complete['title'], epochs=500, total_examples=title_w2v.corpus_count)\n",
    "    cache.save(title_w2v,'title_w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache.save(description_w2v,'description_w2v')\n",
    "cache.save(title_w2v,'title_w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache.save(content_w2v,'content_w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description_w2v = compute_embedding(clean_df_complete['description'],description_w2v)\n",
    "feature_title_w2v = compute_embedding(clean_df_complete['title'],title_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_content_w2v = compute_embedding(clean_df_complete['content'],content_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "culture             13706\n",
       "politique           13418\n",
       "sport               12063\n",
       "planete             11696\n",
       "economie             8435\n",
       "actualite-medias     7192\n",
       "societe              6701\n",
       "sante                6308\n",
       "campus               5984\n",
       "sciences             5484\n",
       "international        5414\n",
       "Name: thematic, dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df_complete['thematic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(df,column_name,limit_value):\n",
    "    \n",
    "    df2 = pd.DataFrame.copy(df.loc[df[column_name].isin(df[column_name].value_counts().loc[df[column_name].value_counts() >= limit_value].index)])\n",
    "    theme_names = df[column_name].value_counts().loc[df[column_name].value_counts() >= limit_value].index\n",
    "    return (df2,theme_names)\n",
    "    \n",
    "df2,theme_names = filter_df(clean_df_complete,'thematic',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_theme_names = {}\n",
    "reverse_dic_theme_names = {}\n",
    "for i,item in enumerate(theme_names):\n",
    "    dic_theme_names[item]=i\n",
    "    reverse_dic_theme_names[i]=item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'culture': 0,\n",
       " 'politique': 1,\n",
       " 'sport': 2,\n",
       " 'planete': 3,\n",
       " 'economie': 4,\n",
       " 'actualite-medias': 5,\n",
       " 'societe': 6,\n",
       " 'sante': 7,\n",
       " 'campus': 8,\n",
       " 'sciences': 9,\n",
       " 'international': 10}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_theme_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(nom):\n",
    "    return dic_theme_names[nom]\n",
    "\n",
    "labels = pd.DataFrame(df2['thematic'].apply(lambda x : f1(x))).rename(index=str, columns={\"thematic\": \"thematic_value\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['article_id'] = [i for i in range(len(labels))]\n",
    "labels['thematic'] = [df2['thematic'][i] for i in range(len(labels))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels[['article_id','thematic','thematic_value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96401, 3)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache.save(feature_description_w2v,'feature_description_w2v_96k')\n",
    "cache.save(feature_title_w2v,'feature_title_w2v_96k')\n",
    "cache.save(labels,'labels_96k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache.save(feature_content_w2v,'feature_content_w2v_96k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\auges\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "import nltk.internals\n",
    "from gensim import corpora, models\n",
    "\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = clean_df_complete['title'].values\n",
    "dictionary = corpora.Dictionary([d.split() for d in corpus])\n",
    "corpus_gensim = [dictionary.doc2bow(doc.split()) for doc in corpus]   \n",
    "tfidf = TfidfModel(corpus_gensim)\n",
    "corpus_tfidf = tfidf[corpus_gensim]\n",
    "lsi = LsiModel(corpus_tfidf, id2word=dictionary, num_topics=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2Vec_LSA(text):\n",
    "    vec = dictionary.doc2bow(text.lower().split())   \n",
    "    vec_lsi = lsi[vec]\n",
    "    return np.array([a[1] for a in vec_lsi])\n",
    "\n",
    "def LSA_dataframe(data, column, n_topics):\n",
    "    LSA = pd.DataFrame(index=data.index, columns=['dim_' + str(i) for i in range(1,n_topics + 1)])\n",
    "    dt = data[column].apply(lambda x: text2Vec_LSA(x))\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_title_lsa = clean_df_complete['title'].apply(lambda x : text2Vec_LSA(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = clean_df_complete['content'].values\n",
    "dictionary = corpora.Dictionary([d.split() for d in corpus])\n",
    "corpus_gensim = [dictionary.doc2bow(doc.split()) for doc in corpus]   \n",
    "tfidf = TfidfModel(corpus_gensim)\n",
    "corpus_tfidf = tfidf[corpus_gensim]\n",
    "lsi = LsiModel(corpus_tfidf, id2word=dictionary, num_topics=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_content_lsa = clean_df_complete['content'].apply(lambda x : text2Vec_LSA(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = clean_df_complete['description'].values\n",
    "dictionary = corpora.Dictionary([d.split() for d in corpus])\n",
    "corpus_gensim = [dictionary.doc2bow(doc.split()) for doc in corpus]   \n",
    "tfidf = TfidfModel(corpus_gensim)\n",
    "corpus_tfidf = tfidf[corpus_gensim]\n",
    "lsi = LsiModel(corpus_tfidf, id2word=dictionary, num_topics=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description_lsa = clean_df_complete['description'].apply(lambda x : text2Vec_LSA(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache.save(feature_content_lsa,'feature_content_lsa_96k')\n",
    "cache.save(feature_title_lsa,'feature_title_lsa_96k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache.save(feature_description_lsa,'feature_description_lsa_96k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using keras NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd.seed(a = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cache_dir/feature_content_w2v_96k'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-180-72b2103e4af6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcontent_lsa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'feature_content_lsa_96k'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtitle_lsa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'feature_title_lsa_96k'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcontent_w2v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'feature_content_w2v_96k'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtitle_w2v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'feature_title_w2v_96k'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\python\\Deep Learning\\projet\\git\\cache.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCACHE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cache_dir/feature_content_w2v_96k'"
     ]
    }
   ],
   "source": [
    "labels  = cache.load('labels_96k')\n",
    "content_lsa = cache.load('feature_content_lsa_96k')\n",
    "title_lsa = cache.load('feature_title_lsa_96k')\n",
    "content_w2v = cache.load('feature_content_w2v_96k')\n",
    "title_w2v = cache.load('feature_title_w2v_96k')\n",
    "\n",
    "Y = labels['thematic_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
