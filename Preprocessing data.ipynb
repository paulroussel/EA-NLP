{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cache import Cache\n",
    "\n",
    "cache=Cache()\n",
    "\n",
    "df = cache.load('dataframe_with_datetime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\auges\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\auges\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "import nltk.internals\n",
    "from gensim import corpora, models\n",
    "\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FrenchStemmer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c47d922bf46c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mstemmer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFrenchStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mstemming\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'FrenchStemmer' is not defined"
     ]
    }
   ],
   "source": [
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words('french')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "\n",
    "stemmer = FrenchStemmer()\n",
    "\n",
    "def stemming(text):\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer(text):\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].apply(lambda x: remove_punct(x))\n",
    "#df['content'] = df['content'].apply(lambda x: tokenization(x.lower()))\n",
    "df['content'] = df['content'].apply(lambda x: remove_stopwords(x))\n",
    "df['content'] = df['content'].apply(lambda x: stemming(x))\n",
    "df['content'] = df['content'].apply(lambda x: lemmatizer(x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA features\n",
    "\n",
    "http://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/\n",
    "\n",
    "Prend en entré df et met dans la base de donnée une nouvelle colonne 'Vec_LSA' qui représente un vecteur, il y a donc une colonne avec un vecteur en tant que valeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df[\"content\"].values\n",
    "dictionary = corpora.Dictionary([d.split() for d in corpus])\n",
    "corpus_gensim = [dictionary.doc2bow(doc.split()) for doc in corpus]   \n",
    "tfidf = TfidfModel(corpus_gensim)\n",
    "corpus_tfidf = tfidf[corpus_gensim]\n",
    "#lsi = LsiModel(corpus_tfidf, id2word=dictionary, num_topics=5)\n",
    "\n",
    "#On cherche ici le nomre optimal de topic entre les nombres: start et stop\n",
    "coherence_values = []\n",
    "model_list = []\n",
    "start,stop,step=2,10,1\n",
    "for num_topics in range(start, stop, step):\n",
    "    # generate LSA model\n",
    "    print(num_topics)\n",
    "    model_Lsi = LsiModel(corpus_tfidf, num_topics=num_topics, id2word = dictionary)  # train model\n",
    "    #model_list.append(model_Lsi) Pour pas refaire le meilleur modèle\n",
    "    coherencemodel = CoherenceModel(model=model_Lsi, texts=[d.split() for d in corpus], dictionary=dictionary, coherence='c_v')\n",
    "    coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "num_topics_optimal = coherence_values.index(max(coherence_values))+ start\n",
    "\n",
    "print(\"nombre optimal de topics:\"+ num_topics_optimal)\n",
    "model_Lsi = LsiModel(corpus_tfidf, num_topics=num_topics_optimal, id2word = dictionary)  # train model\n",
    "\n",
    "def text2Vec_LSA(text):\n",
    "    vec = dictionary.doc2bow(text.lower().split())   \n",
    "    vec_lsi = lsi[vec]\n",
    "    return(vec_lsi)\n",
    "\n",
    "df['Vec_LSA'] = df['content'].apply(lambda x: text2Vec_LSA(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
=======
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
>>>>>>> a1a39cfffc77179484ff2125de6b811bdd993c41
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.6.5"
=======
   "version": "3.6.8"
>>>>>>> a1a39cfffc77179484ff2125de6b811bdd993c41
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
