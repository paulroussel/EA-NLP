{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cache import Cache\n",
    "cache=Cache()\n",
    "\n",
    "df = cache.load('dataframe_with_datetime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "import nltk.internals\n",
    "from gensim import corpora, models\n",
    "\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words('french')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "\n",
    "stemmer = FrenchStemmer()\n",
    "\n",
    "def stemming(text):\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer(text):\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words('french')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "\n",
    "stemmer = FrenchStemmer()\n",
    "\n",
    "def stemming(text):\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer(text):\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].apply(lambda x: remove_punct(x))\n",
    "#df['content'] = df['content'].apply(lambda x: tokenization(x.lower()))\n",
    "df['content'] = df['content'].apply(lambda x: remove_stopwords(x))\n",
    "df['content'] = df['content'].apply(lambda x: stemming(x))\n",
    "df['content'] = df['content'].apply(lambda x: lemmatizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df[\"content\"].values\n",
    "dictionary = corpora.Dictionary([d.split() for d in corpus])\n",
    "corpus_gensim = [dictionary.doc2bow(doc.split()) for doc in corpus]   \n",
    "tfidf = TfidfModel(corpus_gensim)\n",
    "corpus_tfidf = tfidf[corpus_gensim]\n",
    "#lsi = LsiModel(corpus_tfidf, id2word=dictionary, num_topics=5)\n",
    "\n",
    "#On cherche ici le nomre optimal de topic entre les nombres: start et stop\n",
    "coherence_values = []\n",
    "model_list = []\n",
    "start,stop,step=2,10,1\n",
    "for num_topics in range(start, stop, step):\n",
    "    # generate LSA model\n",
    "    print(num_topics)\n",
    "    model_Lsi = LsiModel(corpus_tfidf, num_topics=num_topics, id2word = dictionary)  # train model\n",
    "    #model_list.append(model_Lsi) Pour pas refaire le meilleur modèle\n",
    "    coherencemodel = CoherenceModel(model=model_Lsi, texts=[d.split() for d in corpus], dictionary=dictionary, coherence='c_v')\n",
    "    coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "num_topics_optimal = coherence_values.index(max(coherence_values))+ start\n",
    "\n",
    "print(\"nombre optimal de topics:\"+ num_topics_optimal)\n",
    "model_Lsi = LsiModel(corpus_tfidf, num_topics=num_topics_optimal, id2word = dictionary)  # train model\n",
    "\n",
    "def text2Vec_LSA(text):\n",
    "    vec = dictionary.doc2bow(text.lower().split())   \n",
    "    vec_lsi = lsi[vec]\n",
    "    return(vec_lsi)\n",
    "\n",
    "df['Vec_LSA'] = df['content'].apply(lambda x: text2Vec_LSA(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_list = [\n",
    "    \"corpus = df[\\\"content\\\"].values\\n\",\n",
    "    \"dictionary = corpora.Dictionary([d.split() for d in corpus])\\n\",\n",
    "    \"corpus_gensim = [dictionary.doc2bow(doc.split()) for doc in corpus]   \\n\",\n",
    "    \"tfidf = TfidfModel(corpus_gensim)\\n\",\n",
    "    \"corpus_tfidf = tfidf[corpus_gensim]\\n\",\n",
    "    \"#lsi = LsiModel(corpus_tfidf, id2word=dictionary, num_topics=5)\\n\",\n",
    "    \"\\n\",\n",
    "    \"#On cherche ici le nomre optimal de topic entre les nombres: start et stop\\n\",\n",
    "    \"coherence_values = []\\n\",\n",
    "    \"model_list = []\\n\",\n",
    "    \"start,stop,step=2,10,1\\n\",\n",
    "    \"for num_topics in range(start, stop, step):\\n\",\n",
    "    \"    # generate LSA model\\n\",\n",
    "    \"    print(num_topics)\\n\",\n",
    "    \"    model_Lsi = LsiModel(corpus_tfidf, num_topics=num_topics, id2word = dictionary)  # train model\\n\",\n",
    "    \"    #model_list.append(model_Lsi) Pour pas refaire le meilleur modèle\\n\",\n",
    "    \"    coherencemodel = CoherenceModel(model=model_Lsi, texts=[d.split() for d in corpus], dictionary=dictionary, coherence='c_v')\\n\",\n",
    "    \"    coherence_values.append(coherencemodel.get_coherence())\\n\",\n",
    "    \"\\n\",\n",
    "    \"num_topics_optimal = coherence_values.index(max(coherence_values))+ start\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"nombre optimal de topics:\\\"+ num_topics_optimal)\\n\",\n",
    "    \"model_Lsi = LsiModel(corpus_tfidf, num_topics=num_topics_optimal, id2word = dictionary)  # train model\\n\",\n",
    "    \"\\n\",\n",
    "    \"def text2Vec_LSA(text):\\n\",\n",
    "    \"    vec = dictionary.doc2bow(text.lower().split())   \\n\",\n",
    "    \"    vec_lsi = lsi[vec]\\n\",\n",
    "    \"    return(vec_lsi)\\n\",\n",
    "    \"\\n\",\n",
    "    \"df['Vec_LSA'] = df['content'].apply(lambda x: text2Vec_LSA(x))\"\n",
    "   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus = df[\"content\"].values\n",
      "dictionary = corpora.Dictionary([d.split() for d in corpus])\n",
      "corpus_gensim = [dictionary.doc2bow(doc.split()) for doc in corpus]   \n",
      "tfidf = TfidfModel(corpus_gensim)\n",
      "corpus_tfidf = tfidf[corpus_gensim]\n",
      "#lsi = LsiModel(corpus_tfidf, id2word=dictionary, num_topics=5)\n",
      "\n",
      "#On cherche ici le nomre optimal de topic entre les nombres: start et stop\n",
      "coherence_values = []\n",
      "model_list = []\n",
      "start,stop,step=2,10,1\n",
      "for num_topics in range(start, stop, step):\n",
      "    # generate LSA model\n",
      "    print(num_topics)\n",
      "    model_Lsi = LsiModel(corpus_tfidf, num_topics=num_topics, id2word = dictionary)  # train model\n",
      "    #model_list.append(model_Lsi) Pour pas refaire le meilleur modèle\n",
      "    coherencemodel = CoherenceModel(model=model_Lsi, texts=[d.split() for d in corpus], dictionary=dictionary, coherence='c_v')\n",
      "    coherence_values.append(coherencemodel.get_coherence())\n",
      "\n",
      "num_topics_optimal = coherence_values.index(max(coherence_values))+ start\n",
      "\n",
      "print(\"nombre optimal de topics:\"+ num_topics_optimal)\n",
      "model_Lsi = LsiModel(corpus_tfidf, num_topics=num_topics_optimal, id2word = dictionary)  # train model\n",
      "\n",
      "def text2Vec_LSA(text):\n",
      "    vec = dictionary.doc2bow(text.lower().split())   \n",
      "    vec_lsi = lsi[vec]\n",
      "    return(vec_lsi)\n",
      "\n",
      "df['Vec_LSA'] = df['content'].apply(lambda x: text2Vec_LSA(x))\n"
     ]
    }
   ],
   "source": [
    "string = \"\"\n",
    "for elem in string_list:\n",
    "    string+= elem\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
